'''
上面已经提到，整个训练的目的，就是最小化整个网络的误差
现在整个神经网络，有h1神经元，其权重分别为w1,w2,截距为b1
h2神经元，其权重分别为w3,w4,截距为b2；o1神经元，是最后的输出，其权重分别为w5,w6,截距为b3

由于损失MSE当中，有Ypred（预测值），所以损失是Ypred（预测值）的函数
又因为Ypred=o1=f(w5*h1+w6*h2+b3)，又因为h1，h2中，又包括了w1,w2,截距为b1，w3,w4,截距为b2
所以整个神经网络的损失L可以记录为整个权重和所有截距的函数，即
L(w1,w2,w3,w4,w5,w6,b1,b2,b3)

假设我们要优化w1，当我们改变w1时，损失L会怎么变化？这涉及到多元微分中的偏导数，研究一个量变化如何结果的
即d(L)/d(w1),记为偏差对w1的偏导数

然后以一个实际例子，即Alice为例子，经过一系列的换算，可以得出round(L)/round(w1)的结果为0.0214，是正数
这也意味着，增加w1,误差L也会随之轻微上升。
然后使用【随机梯度下降法】的优化算法来优化网络的权重和截距项，实现损失的最小化。核心就是这个更新等式
w1=w1-n*(L对w1的偏导数)，n是一个常数，被称为学习率，用于调整训练的速度。
如果n*(L对w1的偏导数)是正数，那么w1会减小，L会下降
如果n*(L对w1的偏导数)是负数，那么w1会增大，L会增大
如果我们对网络中的每个权重和截距项都这样进行优化，损失就会不断下降，网络性能会不断上升

我们的训练过程是这样的：
1.从我们的数据集中选择一个样本，用随机梯度下降法进行优化――每次我们都只针对一个样本进行优化；
2.计算每个权重或截距项对损失的偏导（例如round(L)/round(w1)，round(L)/round(w2)等)；
3.用更新等式更新每个权重和截距项；
4.重复第一步；
'''
